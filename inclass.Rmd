---
title: "in class"
output: html_document
date: "2025-01-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 01-16-25

## Standard normal distributions

Verifying- example with in class R commands

```{r}
#default is standard normal if you dont specify mean and sd
qnorm(0.025)
qnorm(0.975)
```

As shown, within 2 sd of the mean we should observe around 95% of the data (0.025-0.975)

# 01/21/25

```{r}
#install.packages("ICSNP")
library("ICSNP")

data("LASERI")
head(LASERI)
#we want to compare different indicators- cardiac output before and after tilt therapy
#focusing on 4 different measures for this - 
laseri <- LASERI[,25:28]

#looking at pair plot will show how these four variables relate to eachother
pairs(laseri)

```

what is the true mean difference between the pre and post tilt for each of the four measures of differences? can we formally assess whether there is a change between pre and post for all these measures? is the tilting experiment having a healthy impact?

```{r}

```

## 1/23/25

univariate hyp test- manual way-

```{r}
mu0 <- 0 #null value 

n<- nrow(LASERI)
alpha <- 0.05

#t crit value using built in r function 
z.crit <- qnorm(p=alpha/2,lower.tail=F)

xbar =mean(laseri$HRT1T4)
stddev = sd(laseri$HRT1T4)
test.stat <- (xbar-mu0)/(stddev/sqrt(n))
pval<- 2*pnorm(abs(test.stat),lower.tail = FALSE)
pval #significant 
```

built in t test function -

```{r}
t.test(laseri$HRT1T4)
```

-   note if the df (n-1) wasnt large enough, you would need to check normality. here it is large enough

-   note 95% CIs are given automatically here as well

# 2/27/25

HW4- question 3 on LDA analysis work already done on it:

```{r}
firms <- read.table("data/T11-4.DAT", header = FALSE)
colnames(firms) <- c( paste0("X", 1:4), "status")
#head(firms)

#install.packages("MASS")
library(MASS)

# Load necessary library
library(MASS)

# Assuming you have the dataset 'firms' as a data frame
# Fit the LDA model
lda_model <- lda(status ~ X1 + X2 + X3 + X4, data = firms)

#coefficients (a) and the intercept (b)
lda_model$coefficients
lda_model$prior  #priors
lda_model$means  #means of each class

# The coefficients correspond to 'a'
# The threshold value 'b' is the intercept for each class
```

(a) a is 0.457 and b is 0.543

(b) confusion matrix and APER

```{r}
predictions <- predict(lda_model)$class

#confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = firms$status)

print(conf_matrix)

#compute APER
incorrect_classifications <- sum(predictions != firms$status)
total_classifications <- length(firms$status)

aper <- incorrect_classifications / total_classifications
print(paste("APER:", aper))
```

(c) Potential problems are overfitting because the CM is computed on the training data itself, thus it might not be generalizable to unseen data. It also might be a biased estimate due to using the entire dataset for training and evaluation leading to an over optimistic estimate of performance.

```{r}
set.seed(123)

#split the dataset into training (70%) and test (30%) sets
train_index <- sample(1:nrow(firms), size = 0.7 * nrow(firms))
train_data <- firms[train_index, ]
test_data <- firms[-train_index, ]

#fit the LDA model on the training set
lda_model_train <- lda(status ~ X1 + X2 + X3 + X4, data = train_data)

#predict on the test set
predictions_test <- predict(lda_model_train, newdata = test_data)$class

#create the confusion matrix for the test set
conf_matrix_test <- table(Predicted = predictions_test, Actual = test_data$status)
print(conf_matrix_test)

#compute the error rate on the test set
incorrect_classifications_test <- sum(predictions_test != test_data$status)
total_classifications_test <- length(test_data$status)

#error rate (1 - accuracy)
error_rate_test <- incorrect_classifications_test / total_classifications_test
print(paste("Error Rate (Hold-Out Method):", error_rate_test))

```

This rate is slightly higher than the original rate, but is likely more accurate because it does not have overfitting bias.

(d) ----------------------- NOT DONE YET
